!mkdir -p LiteTTP-LLM/{notebooks,src,data,figures}


!pip install -q transformers==4.39.3 torch scikit-learn pandas numpy nltk matplotlib tqdm


from google.colab import files
files.upload()


import pandas as pd

df = pd.read_csv("/content/MITRE_ATT&CK_Dataset.csv")

# normalize column names
df.columns = (
    df.columns
      .str.strip()
      .str.lower()
      .str.replace(" ", "_")
)

print(df.columns.tolist())


df = df.rename(columns={
    "sentences": "sentence"
})

df.head()


import re

def clean_text(s):
    s = str(s).lower()
    s = re.sub(r"\b\d+\b", " ", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s

df["sentence"] = df["sentence"].apply(clean_text)


grouped = df.groupby("sentence")["technique_id"].apply(list).reset_index()

X = grouped["sentence"]
y = grouped["technique_id"]

print(X.head())
print(y.head())


%%writefile LiteTTP-LLM/src/preprocessing.py
import re
import nltk
nltk.download("punkt")
from nltk.tokenize import sent_tokenize

def clean_text(text):
    text = text.lower()
    text = re.sub(r"\b\d+\b", " ", text)
    text = re.sub(r"\s+", " ", text).strip()
    return text

def split_into_sentences(text):
    return [clean_text(s) for s in sent_tokenize(text) if len(s.strip()) > 5]


%%writefile LiteTTP-LLM/src/metrics.py
from sklearn.metrics import precision_score, recall_score, f1_score, hamming_loss

def multilabel_metrics(y_true, y_pred):
    return {
        "precision": precision_score(y_true, y_pred, average="micro", zero_division=0),
        "recall": recall_score(y_true, y_pred, average="micro", zero_division=0),
        "f1": f1_score(y_true, y_pred, average="micro", zero_division=0),
        "hamming": hamming_loss(y_true, y_pred)
    }


import sys
sys.path.append("LiteTTP-LLM")

import pandas as pd
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.multiclass import OneVsRestClassifier

from src.preprocessing import clean_text
from src.metrics import multilabel_metrics


import sys
sys.path.append("LiteTTP-LLM")

import pandas as pd
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.multiclass import OneVsRestClassifier

from src.preprocessing import clean_text
from src.metrics import multilabel_metrics

# --- reload + normalize columns again (THIS IS THE FIX) ---
df = pd.read_csv("/content/MITRE_ATT&CK_Dataset.csv")

df.columns = (
    df.columns
      .str.strip()
      .str.lower()
      .str.replace(" ", "_")
)

# rename plural sentences -> sentence
if "sentences" in df.columns and "sentence" not in df.columns:
    df = df.rename(columns={"sentences": "sentence"})

# sanity check
print("Columns:", df.columns.tolist())
assert "sentence" in df.columns, "❌ 'sentence' column missing after normalization"
assert "technique_id" in df.columns, "❌ 'technique_id' column missing after normalization"

# now safe
df["sentence"] = df["sentence"].astype(str).apply(clean_text)
df.head()


grouped = df.groupby("sentence")["technique_id"].apply(list).reset_index()
X = grouped["sentence"]
y = grouped["technique_id"]

mlb = MultiLabelBinarizer()
y_bin = mlb.fit_transform(y)
print("✅ Samples:", len(X), " Labels:", len(mlb.classes_))


X_train, X_test, y_train, y_test = train_test_split(
    X, y_bin, test_size=0.2, random_state=42
)


import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.multiclass import OneVsRestClassifier

# Vectorize text
vectorizer = TfidfVectorizer(max_features=20000, ngram_range=(1,2))
X_train_vec = vectorizer.fit_transform(X_train)
X_test_vec  = vectorizer.transform(X_test)

# Train multi-label classifier
clf = OneVsRestClassifier(LogisticRegression(max_iter=2000))
clf.fit(X_train_vec, y_train)

# ---------- 1) DEFAULT prediction (for comparison) ----------
y_pred_default = clf.predict(X_test_vec)
m_default = multilabel_metrics(y_test, y_pred_default)

print("✅ Default Metrics:", m_default)

# ---------- 2) THRESHOLD-TUNED prediction (recommended) ----------
# Use predicted probabilities (works with OneVsRestClassifier)
y_scores = clf.predict_proba(X_test_vec)  # shape: [n_samples, n_labels]

def predict_with_threshold(scores, tau=0.25, max_L=3):
    """
    scores: probability matrix [n_samples, n_labels]
    tau: threshold for selecting labels
    max_L: maximum labels allowed per sample (top-k)
    """
    y_pred = np.zeros_like(scores, dtype=int)

    for i in range(scores.shape[0]):
        row = scores[i]

        # indices above threshold
        idx = np.where(row >= tau)[0]

        # ensure at least one label
        if len(idx) == 0:
            idx = np.array([np.argmax(row)])

        # keep top max_L among selected
        idx = idx[np.argsort(row[idx])[::-1]][:max_L]

        y_pred[i, idx] = 1

    return y_pred

# You can adjust these two values to change precision/recall
TAU = 0.25
MAX_L = 3

y_pred_tuned = predict_with_threshold(y_scores, tau=TAU, max_L=MAX_L)
m_tuned = multilabel_metrics(y_test, y_pred_tuned)

print(f"✅ Tuned Metrics (tau={TAU}, max_L={MAX_L}):", m_tuned)


import matplotlib.pyplot as plt
import numpy as np

# Assuming you already have these dictionaries from your previous cell:
# m_default = {"precision":..., "recall":..., "f1":..., "hamming":...}
# m_tuned   = {"precision":..., "recall":..., "f1":..., "hamming":...}

metrics_names = ["precision", "recall", "f1", "hamming"]
default_vals = [m_default[k] for k in metrics_names]
tuned_vals   = [m_tuned[k] for k in metrics_names]

x = np.arange(len(metrics_names))
width = 0.35

plt.figure(figsize=(9,4))
plt.bar(x - width/2, default_vals, width, label="Default")
plt.bar(x + width/2, tuned_vals, width, label=f"Tuned (tau={TAU}, max_L={MAX_L})")

plt.xticks(x, ["Precision", "Recall", "F1", "Hamming"])
plt.ylabel("Score")
plt.title("Multi-Label Baseline: Default vs Tuned Thresholding")
plt.legend()
plt.grid(True, axis="y", linestyle="--", alpha=0.4)
plt.show()
